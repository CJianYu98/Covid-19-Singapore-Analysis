{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.0 64-bit",
   "metadata": {
    "interpreter": {
     "hash": "7812ea015bdcee6f23a998adcdd2ef97c151c0c241b7b7070987d9313e41299d"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Basic Imports\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from itertools import combinations\n",
    "\n",
    "### Graphic libraries\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns \n",
    "\n",
    "### Importing text processing packages\n",
    "from nltk.corpus import PlaintextCorpusReader\n",
    "from nltk import word_tokenize\n",
    "from valuable_features import *\n",
    "\n",
    "### Importing the relevant ML libraries \n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.tree import DecisionTreeClassifier, plot_tree\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn import svm\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import mean_squared_error, roc_auc_score\n",
    "from sklearn.model_selection import StratifiedKFold, cross_validate, cross_val_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score,roc_curve,auc,recall_score,f1_score,precision_score,classification_report,confusion_matrix,auc\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.utils import resample"
   ]
  },
  {
   "source": [
    "# Creating features for comment (if necessary)\n",
    "\n",
    "## Read in training and validation data first"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_parent = os.path.dirname(os.path.dirname(os.getcwd()))\n",
    "training_data_path = f'{path_parent}/Data/Thoughtful Comments/thoughtful_comments_labelled.csv'\n",
    "validation_data_path = f'{path_parent}/Data/Thoughtful Comments/validation_comments.csv'"
   ]
  },
  {
   "source": [
    "## Create features\n",
    "### Function for creating features in valuable_features.py"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_features_created = create_features(training_data_path)\n",
    "training_features_created.to_csv(f'{path_parent}/Data/Thoughtful Comments/thoughtful_comments_final.csv')\n",
    "\n",
    "validation_features_created = create_features(validation_data_path)\n",
    "validation_features_created.to_csv(f'{path_parent}/Data/Thoughtful Comments/validation_comments_final.csv')"
   ]
  },
  {
   "source": [
    "# Reading training and validation data after features creation for EDA and modelling"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_parent = os.path.dirname(os.path.dirname(os.getcwd()))\n",
    "training_df = pd.read_csv(f'{path_parent}/Data/Thoughtful Comments/thoughtful_comments_final.csv')\n",
    "validation_df = pd.read_csv(f'{path_parent}/Data/Thoughtful Comments/validation_comments_final.csv')"
   ]
  },
  {
   "source": [
    "# EDA"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_df[['Length Category']].value_counts()\n",
    "training_df[['Relevance Score Category']].value_counts()"
   ]
  },
  {
   "source": [
    "## Plotting boxplots for all features to see if there is significant difference between valuable/not valuable comments"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### Feature 1 (Length)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_theme(style='whitegrid')\n",
    "fig, ax = plt.subplots()\n",
    "fig.set_size_inches(10, 8)\n",
    "sns.boxplot(data=training_df, y='Length', x='Thoughtful?', ax=ax).set_title('Boxplot of Length (with outliers)', fontsize=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_theme(style='whitegrid')\n",
    "fig, ax = plt.subplots()\n",
    "fig.set_size_inches(10, 8)\n",
    "sns.boxplot(data=training_df, y='Length', x='Thoughtful?', ax=ax, showfliers=False).set_title('Boxplot of Length (without outliers)', fontsize=20)"
   ]
  },
  {
   "source": [
    "### Feature 2 (Comment Likelihood)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_theme(style='whitegrid')\n",
    "fig, ax = plt.subplots()\n",
    "fig.set_size_inches(10, 8)\n",
    "sns.boxplot(data=training_df, y='Average Loglikelihood', x='Thoughtful?', ax=ax).set_title('Boxplot of Comment Likelihood (with outliers)', fontsize=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_theme(style='whitegrid')\n",
    "fig, ax = plt.subplots()\n",
    "fig.set_size_inches(10, 8)\n",
    "sns.boxplot(data=training_df, y='Average Loglikelihood', x='Thoughtful?', ax=ax, showfliers=False).set_title('Boxplot of Comment Likelihood (without outliers)', fontsize=20)"
   ]
  },
  {
   "source": [
    "### Feature 3 (Num Verbs)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_theme(style='whitegrid')\n",
    "fig, ax = plt.subplots()\n",
    "fig.set_size_inches(10, 8)\n",
    "sns.boxplot(data=training_df, y='Num Verbs', x='Thoughtful?', ax=ax).set_title('Boxplot of Number of Verbs (with outliers)', fontsize=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_theme(style='whitegrid')\n",
    "fig, ax = plt.subplots()\n",
    "fig.set_size_inches(10, 8)\n",
    "sns.boxplot(data=training_df, y='Num Verbs', x='Thoughtful?', ax=ax, showfliers=False).set_title('Boxplot of Number of Verbs (without outliers)', fontsize=20)"
   ]
  },
  {
   "source": [
    "### Feature 4 (Num Discourse Relations)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_theme(style='whitegrid')\n",
    "fig, ax = plt.subplots()\n",
    "fig.set_size_inches(10, 8)\n",
    "sns.boxplot(data=training_df, y='Num Discourse Relations', x='Thoughtful?', ax=ax).set_title('Boxplot of Number of Discourse Relations (with outliers)', fontsize=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_theme(style='whitegrid')\n",
    "fig, ax = plt.subplots()\n",
    "fig.set_size_inches(10, 8)\n",
    "sns.boxplot(data=training_df, y='Num Discourse Relations', x='Thoughtful?', ax=ax, showfliers=False).set_title('Boxplot of Number of Discourse Relations (without outliers)', fontsize=20)"
   ]
  },
  {
   "source": [
    "### Feature 5 (Relevance Score)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_theme(style='whitegrid')\n",
    "fig, ax = plt.subplots()\n",
    "fig.set_size_inches(10, 8)\n",
    "sns.boxplot(data=training_df, y='Relevance score', x='Thoughtful?', ax=ax).set_title('Boxplot of Relevance Score (with outliers)', fontsize=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_theme(style='whitegrid')\n",
    "fig, ax = plt.subplots()\n",
    "fig.set_size_inches(10, 8)\n",
    "sns.boxplot(data=training_df, y='Relevance score', x='Thoughtful?', ax=ax, showfliers=False).set_title('Boxplot of Relevance Score (without outliers)', fontsize=20)"
   ]
  },
  {
   "source": [
    "### Feature 6 (Num Pronouns)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_theme(style='whitegrid')\n",
    "fig, ax = plt.subplots()\n",
    "fig.set_size_inches(10, 8)\n",
    "sns.boxplot(data=training_df, y='Num Pronouns', x='Thoughtful?', ax=ax).set_title('Boxplot of Num Pronouns (with outliers)', fontsize=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_theme(style='whitegrid')\n",
    "fig, ax = plt.subplots()\n",
    "fig.set_size_inches(10, 8)\n",
    "sns.boxplot(data=training_df, y='Num Pronouns', x='Thoughtful?', ax=ax, showfliers=False).set_title('Boxplot of Num Pronouns (without outliers)', fontsize=20)"
   ]
  },
  {
   "source": [
    "## Looking at distributions of independent variables"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_list = ['Length Category', 'Average Loglikelihood', 'Num Verbs', 'Num Discourse Relations', 'Relevance score', 'Num Pronouns']\n",
    "target = 'Thoughtful?'\n",
    "\n",
    "# Function to plot distribution graphs\n",
    "def eda_plots(df,features_list,plot_title,fig_size):\n",
    "\n",
    "    df[features_list].hist(bins=40,edgecolor='b',linewidth=1.0,xlabelsize=8,ylabelsize=8,grid= False,figsize=fig_size ,color='red')\n",
    "\n",
    "    plt.tight_layout(rect=(0,0,1.2,1.2))\n",
    "\n",
    "    # Overall title for all the plots\n",
    "    plt.suptitle(plot_title,x=0.65,y=1.25,fontsize=14)\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eda_plots(df,features_list,'Features Univariate Plots',(20,7))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eda_plots(df,target,'Target Variable Univariate Plots',(5,5))"
   ]
  },
  {
   "source": [
    "# Standardizing independent variables"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "\n",
    "training_X_scaled = scaler.fit_transform(training_df[features_list])\n",
    "training_X_scaled = pd.DataFrame(training_X_scaled, columns=features_list)\n",
    "\n",
    "validation_X_scaled = scaler.fit_transform(validation_df[features_list])\n",
    "validation_X_scaled = pd.DataFrame(validation_X_scaled, columns=features_list)"
   ]
  },
  {
   "source": [
    "# Feature importance (using RandomForestClassifier)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = training_X_scaled[['Length Category', 'Average Loglikelihood', 'Num Verbs', 'Num Discourse Relations', 'Relevance score', 'Num Pronouns']]\n",
    "y = training_df['Thoughtful?']\n",
    "feat_labels = ['Length Category', 'Average Loglikelihood', 'Num Verbs', 'Num Discourse Relations', 'Relevance score', 'Num Pronouns']\n",
    "\n",
    "X_train,X_test,y_train,y_test = train_test_split(X, y, test_size=0.3, random_state=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sel_rfc = SelectFromModel(RandomForestClassifier(n_estimators = 500, random_state=10))\n",
    "sel_rfc.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_feat= X_train.columns[(sel_rfc.get_support())]\n",
    "selected_feat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = RandomForestClassifier(n_estimators=500, random_state=10)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "for feature in zip(feat_labels, clf.feature_importances_):\n",
    "    print(feature)"
   ]
  },
  {
   "source": [],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "# Modelling"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Creating a dictionary for all the classification models"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models_dict = {\n",
    "    'Logistic Regression':LogisticRegression(),\n",
    "    'Naive Bayes':GaussianNB(),\n",
    "    # 'Decision Trees':DecisionTreeClassifier(),\n",
    "    'SVM linear': svm.SVC(kernel='linear', probability=True),\n",
    "    'SVM rbf': svm.SVC(kernel='rbf', probability=True),\n",
    "    'Random Forest': RandomForestClassifier(n_estimators = 500, random_state=10),\n",
    "    'XGBoost': xgb.XGBClassifier(use_label_encoder=False)\n",
    "}"
   ]
  },
  {
   "source": [
    "## Train test split on training data"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating our independent and dependent variables df\n",
    "X = training_X_scaled[['Length Category', 'Average Loglikelihood', 'Num Verbs', 'Num Discourse Relations', 'Relevance score', 'Num Pronouns']]\n",
    "y = training_df['Thoughtful?']\n",
    "\n",
    "# Perform train test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=15) \n",
    "\n",
    "# Training with differnent models\n",
    "for model_name in models_dict:\n",
    "    m = models_dict[model_name]\n",
    "    \n",
    "    m.fit(X_train, y_train)\n",
    "    predictions = m.predict(X_test)\n",
    "\n",
    "    acc = accuracy_score(y_test,predictions)  # always true label first, then your predicted labels\n",
    "    precision = precision_score(y_test,predictions) \n",
    "    recall = recall_score(y_test,predictions) \n",
    "    f1 = f1_score(y_test,predictions)\n",
    "\n",
    "    print(model_name)\n",
    "    print('-'*50)\n",
    "    print('Accuracy Score for {} is {:.5f}'.format(model_name,acc))\n",
    "    print('Precision Score for {} is {:.5f}'.format(model_name,precision))\n",
    "    print('Recall Score for {} is {:.5f}'.format(model_name,recall))\n",
    "    print('F1 Score for {} is {:.5f}'.format(model_name,f1))\n",
    "    print()"
   ]
  },
  {
   "source": [
    "## Stratified K-fold Cross Validation"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "skf = StratifiedKFold(n_splits=10)\n",
    "target = training_df['Thoughtful?']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = ['Length Category', 'Average Loglikelihood', 'Num Verbs', 'Num Discourse Relations', 'Relevance score', 'Num Pronouns']\n",
    "y = ['Thoughtful?']\n",
    "\n",
    "model = GaussianNB() # can change the model here\n",
    "\n",
    "fold_no = 1\n",
    "for train_index, test_index in skf.split(df, target):\n",
    "    train = df.loc[train_index,:]\n",
    "    test = df.loc[test_index,:]\n",
    "    print('Fold', str(fold_no), 'Class Ratio:', sum(test['Thoughtful?'])/len(test['Thoughtful?']))\n",
    "    \n",
    "    X_train = train[X]\n",
    "    y_train = train[y]\n",
    "    X_test = test[X]\n",
    "    y_test = test[y]\n",
    "    model.fit(X_train, y_train.values.ravel())\n",
    "    predictions = model.predict(X_test)\n",
    "    print('Fold', str(fold_no), 'F1 score:', f1_score(y_test,predictions))\n",
    "    print()\n",
    "\n",
    "    fold_no += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = training_X_scaled[['Length Category', 'Average Loglikelihood', 'Num Verbs', 'Num Discourse Relations', 'Relevance score', 'Num Pronouns']]\n",
    "y = training_df[['Thoughtful?']]\n",
    "\n",
    "for model_name in models_dict:\n",
    "    m = models_dict[model_name]\n",
    "    scoring = ['accuracy', 'precision', 'recall', 'f1']\n",
    "    scores = cross_validate(m, X.values, y.values.ravel(), scoring=scoring, cv=10)\n",
    "\n",
    "    print(model_name)\n",
    "    print('-'*50)\n",
    "    print(f\"Mean accuracy is {scores['test_accuracy'].mean()}\")\n",
    "    print(f\"Mean precision is {scores['test_precision'].mean()}\")\n",
    "    print(f\"Mean recall is {scores['test_recall'].mean()}\")\n",
    "    print(f\"Mean f1 is {scores['test_f1'].mean()}\")\n",
    "    print()\n",
    "    print()"
   ]
  },
  {
   "source": [
    "## Test against validation data with the best model after stratified k-fold cross validation"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating our independent and dependent variables df\n",
    "X_train = training_X_scaled[['Length Category', 'Average Loglikelihood', 'Num Verbs', 'Num Discourse Relations', 'Relevance score', 'Num Pronouns']]\n",
    "y_train = training_df[['Thoughtful?']]\n",
    "X_test = validation_X_scaled[['Length Category', 'Average Loglikelihood', 'Num Verbs', 'Num Discourse Relations', 'Relevance score', 'Num Pronouns']]\n",
    "y_test = validation_df[['Thoughtful?']]\n",
    "\n",
    "# Training with differnent models\n",
    "for model_name in models_dict:\n",
    "    m = models_dict[model_name]\n",
    "\n",
    "    # X_train = training_data[X]\n",
    "    # y_train = training_data[y]\n",
    "    # X_test = validation_data[X]\n",
    "    # y_test = validation_data[y]\n",
    "    \n",
    "    m.fit(X_train, y_train.values.ravel())\n",
    "    predictions = m.predict(X_test)\n",
    "\n",
    "    acc = accuracy_score(y_test,predictions)  # always true label first, then your predicted labels!\n",
    "    precision = precision_score(y_test,predictions) \n",
    "    recall = recall_score(y_test,predictions) \n",
    "    f1 = f1_score(y_test,predictions)\n",
    "\n",
    "    print(model_name)\n",
    "    print('-'*50)\n",
    "    print('Accuracy Score for {} is {:.5f}'.format(model_name,acc))\n",
    "    print('Precision Score for {} is {:.5f}'.format(model_name,precision))\n",
    "    print('Recall Score for {} is {:.5f}'.format(model_name,recall))\n",
    "    print('F1 Score for {} is {:.5f}'.format(model_name,f1))\n",
    "    print()"
   ]
  },
  {
   "source": [
    "### ROC score"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GaussianNB() #change this based on the best model performance\n",
    "prob = model.predict_proba(X_test)\n",
    "prob = [p[1] for p in prob_no_unsampled]\n",
    "print(roc_auc_score(y_test, prob_no_unsampled))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_fpr, lr_tpr, _ = roc_curve(y_test, prob)\n",
    "plt.plot(lr_fpr, lr_tpr, marker='.', label='SVM (RBF kernal)') #change the label to the model with the best performance\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "source": [
    "## Perform upsampling method and model again"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_thoughtful = training_df[training_df['Thoughtful?'] == 1]\n",
    "training_unthoughtful = training_df[training_df['Thoughtful?'] == 0]\n",
    "\n",
    "scaler = StandardScaler()\n",
    "features_list = ['Length Category', 'Average Loglikelihood', 'Num Verbs', 'Num Discourse Relations', 'Relevance score', 'Num Pronouns']\n",
    "\n",
    "training_thoughtful_X_scaled = scaler.fit_transform(training_thoughtful[features_list])\n",
    "training_thoughtful_X_scaled = pd.DataFrame(training_thoughtful_X_scaled, columns=features_list)\n",
    "training_unthoughtful_X_scaled = scaler.fit_transform(training_unthoughtful[features_list])\n",
    "training_unthoughtful_X_scaled = pd.DataFrame(training_unthoughtful_X_scaled, columns=features_list)\n",
    "\n",
    "training_thoughtful_X_scaled['Thoughtful?'] = training_thoughtful['Thoughtful?']\n",
    "training_unthoughtful_X_scaled['Thoughtful?'] = training_unthoughtful['Thoughtful?']\n",
    "\n",
    "print(len(training_thoughtful_X_scaled[))\n",
    "print(len(training_unthoughtful_X_scaled[))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_thoughtful_upsampled = resample(training_thoughtful_X_scaled, replace=True, n_samples=1000, random_state=170)\n",
    "training_upsampled = pd.concat([training_unthoughtful_X_scaled, training_thoughtful_upsampled])"
   ]
  },
  {
   "source": [
    "### Testing on validation data after upsampling"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating our independent and dependent variables df\n",
    "X_train = training_upsampled[['Length Category', 'Average Loglikelihood', 'Num Verbs', 'Num Discourse Relations', 'Relevance score', 'Num Pronouns']]\n",
    "y_train = training_upsampled[['Thoughtful?']]\n",
    "X_test = validation_X_scaled[['Length Category', 'Average Loglikelihood', 'Num Verbs', 'Num Discourse Relations', 'Relevance score', 'Num Pronouns']]\n",
    "y_test = validation_df[['Thoughtful?']]\n",
    "\n",
    "# Training with differnent models\n",
    "for model_name in models_dict:\n",
    "    m = models_dict[model_name]\n",
    "\n",
    "    # X_train = training_data[X]\n",
    "    # y_train = training_data[y]\n",
    "    # X_test = validation_data[X]\n",
    "    # y_test = validation_data[y]\n",
    "    \n",
    "    m.fit(X_train, y_train.values.ravel())\n",
    "    predictions = m.predict(X_test)\n",
    "\n",
    "    acc = accuracy_score(y_test,predictions)  # always true label first, then your predicted labels!\n",
    "    precision = precision_score(y_test,predictions) \n",
    "    recall = recall_score(y_test,predictions) \n",
    "    f1 = f1_score(y_test,predictions)\n",
    "\n",
    "    print(model_name)\n",
    "    print('-'*50)\n",
    "    print('Accuracy Score for {} is {:.5f}'.format(model_name,acc))\n",
    "    print('Precision Score for {} is {:.5f}'.format(model_name,precision))\n",
    "    print('Recall Score for {} is {:.5f}'.format(model_name,recall))\n",
    "    print('F1 Score for {} is {:.5f}'.format(model_name,f1))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_features_combi(features, n):\n",
    "    combi = []\n",
    "    for i in range(3, n+1):\n",
    "        temp = list(combinations(features, i))\n",
    "        combi += temp\n",
    "    return combi\n",
    "\n",
    "# comb = get_features_combi(['Length Category', 'Average Loglikelihood', 'Num Verbs', 'Num Discourse Relations', 'Relevance Score Category'], 5)\n",
    "combi = get_features_combi(['Length Category', 'Average Loglikelihood', 'Num Verbs', 'Num Discourse Relations', 'Relevance score', 'Num Pronouns'], 6)"
   ]
  },
  {
   "source": [
    "## Test with all possible features combination to find the best model (on validation data)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "scores = {\n",
    "    'features': None, \n",
    "    'model': None,\n",
    "    'acc': 0, \n",
    "    'prec': 0,\n",
    "    'rec': 0,\n",
    "    'f1': 0\n",
    "}\n",
    "\n",
    "for features in comb:\n",
    "    # Creating our independent and dependent variables df\n",
    "    X = list(features)\n",
    "    y = ['Thoughtful?']\n",
    "\n",
    "    # Training with differnent models\n",
    "    for model_name in models_dict:\n",
    "        m = models_dict[model_name]\n",
    "\n",
    "        X_train = training_upsampled[X]\n",
    "        y_train = training_upsampled[y]\n",
    "        X_test = validation_X_scaled[X]\n",
    "        y_test = validation_df[y]\n",
    "        \n",
    "        m.fit(X_train, y_train.values.ravel())\n",
    "        predictions = m.predict(X_test)\n",
    "\n",
    "        acc = accuracy_score(y_test,predictions)  # always true label first, then your predicted labels!\n",
    "        precision = precision_score(y_test,predictions) \n",
    "        recall = recall_score(y_test,predictions) \n",
    "        f1 = f1_score(y_test,predictions)\n",
    "\n",
    "        if f1 > scores['f1']:\n",
    "            scores['features'] = features\n",
    "            scores['model'] = model_name\n",
    "            scores['acc'] = acc\n",
    "            scores['prec'] = precision\n",
    "            scores['rec'] = recall\n",
    "            scores['f1'] = f1\n",
    "scores"
   ],
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "source": [
    "### ROC score after upsampling"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model1 = GaussianNB() #change this based on the best model performance\n",
    "prob_upsampled = model1.predict_proba(X_test)\n",
    "prob_upsampled = [p[1] for p in prob_upsampled]\n",
    "print(roc_auc_score(y_test, prob_upsampled))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_fpr, lr_tpr, _ = roc_curve(y_test, prob_upsampled)\n",
    "plt.plot(lr_fpr, lr_tpr, marker='.', label='SVM (RBF kernal)') #change the label name based on the best performing model\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  }
 ]
}