{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.0 64-bit",
   "metadata": {
    "interpreter": {
     "hash": "7812ea015bdcee6f23a998adcdd2ef97c151c0c241b7b7070987d9313e41299d"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import RegexpTokenizer, sent_tokenize\n",
    "from typing import Iterator\n",
    "\n",
    "\n",
    "def replace_characters(text: str) -> str:\n",
    "    \"\"\"\n",
    "    Replace tricky punctuations that can mess up sentence tokenizers\n",
    "    :param text: text with non-standard punctuations\n",
    "    :return: text with standardized punctuations\n",
    "    \"\"\"\n",
    "    replacement_rules = {'‚Äú': '\"', '‚Äù': '\"', '‚Äô': \"'\", '--': ','}\n",
    "    for symbol, replacement in replacement_rules.items():\n",
    "        text = text.replace(symbol, replacement)\n",
    "    return text\n",
    "\n",
    "\n",
    "def generate_tokenized_sentences(paragraph: str) -> Iterator[str]:\n",
    "    \"\"\"\n",
    "    Tokenize each sentence in paragraph.\n",
    "    For each sentence, tokenize each words and return the tokenized sentence one at a time.\n",
    "    :param paragraph: text of paragraph\n",
    "    \"\"\"\n",
    "    word_tokenizer = RegexpTokenizer(r'[-\\'\\w]+')\n",
    "\n",
    "    for sentence in sent_tokenize(paragraph):\n",
    "        tokenized_sentence = word_tokenizer.tokenize(sentence)\n",
    "        if tokenized_sentence:\n",
    "            tokenized_sentence.append('[END]')\n",
    "            yield tokenized_sentence\n",
    "\n",
    "\n",
    "def tokenize_raw_text(raw_text_path: str, token_text_path: str) -> None:\n",
    "    \"\"\"\n",
    "    Read a input text file and write its content to an output text file in the form of tokenized sentences\n",
    "    :param raw_text_path: path of raw input text file\n",
    "    :param token_text_path: path of tokenized output text file\n",
    "    \"\"\"\n",
    "    with open(raw_text_path) as read_handle, open(token_text_path, 'w') as write_handle:\n",
    "        for paragraph in read_handle:\n",
    "            paragraph = paragraph.lower()\n",
    "            paragraph = replace_characters(paragraph)\n",
    "\n",
    "            for tokenized_sentence in generate_tokenized_sentences(paragraph):\n",
    "                write_handle.write(','.join(tokenized_sentence))\n",
    "                write_handle.write('\\n')\n",
    "\n",
    "\n",
    "def get_tokenized_sentences(file_name: str) -> Iterator[str]:\n",
    "    \"\"\"\n",
    "    Return tokenized sentence one at a time from a tokenized text\n",
    "    :param file_name: path of tokenized text\n",
    "    \"\"\"\n",
    "    # with open(file_name) as file_handle:\n",
    "    #     for sentence in file_handle.read().splitlines():\n",
    "    #         tokenized_sentence = sentence.split(',')\n",
    "    #         yield tokenized_sentence\n",
    "\n",
    "    for sent in file_name:\n",
    "        tokenized_sentence = sent.split(',')\n",
    "        yield tokenized_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import log2\n",
    "\n",
    "\n",
    "class UnigramCounter:\n",
    "    def __init__(self, file_name: str) -> None:\n",
    "        \"\"\"\n",
    "        Initialize unigram counter from tokenized text and count number of unigrams in text\n",
    "        :param file_name: path of tokenized text. Each line is a sentence with tokens separated by comma.\n",
    "        \"\"\"\n",
    "        self.sentence_generator = get_tokenized_sentences(file_name)\n",
    "        self.count()\n",
    "\n",
    "    def count(self) -> None:\n",
    "        \"\"\"\n",
    "        Count number of unigrams in text, one sentence at a time\n",
    "        \"\"\"\n",
    "        self.sentence_count = 0\n",
    "        self.token_count = 0\n",
    "        self.counts = {}\n",
    "\n",
    "        for sentence in self.sentence_generator:\n",
    "            self.sentence_count += 1\n",
    "            self.token_count += len(sentence)\n",
    "            for unigram in sentence:\n",
    "                self.counts[unigram] = self.counts.get(unigram, 0) + 1\n",
    "\n",
    "\n",
    "class UnigramModel:\n",
    "    def __init__(self, train_counter: UnigramCounter) -> None:\n",
    "        \"\"\"\n",
    "        Initialize unigram model from unigram counter, count the number of unique unigrams (vocab)\n",
    "        :param train_counter: counted unigram counter\n",
    "        \"\"\"\n",
    "        self.counter = train_counter\n",
    "        self.counts = train_counter.counts.copy()\n",
    "        self.counts['[UNK]'] = 0\n",
    "        self.vocab = set(self.counts.keys())\n",
    "        self.vocab_size = len(self.vocab)\n",
    "\n",
    "    def train(self, k: int = 1) -> None:\n",
    "        \"\"\"\n",
    "        For each unigram in the vocab, calculate its probability in the text\n",
    "        :param k: smoothing pseudo-count for each unigram\n",
    "        \"\"\"\n",
    "        self.probs = {}\n",
    "        for unigram, unigram_count in self.counts.items():\n",
    "            prob_nom = unigram_count + k\n",
    "            prob_denom = self.counter.token_count + k * self.vocab_size\n",
    "            self.probs[unigram] = prob_nom / prob_denom\n",
    "\n",
    "    def evaluate(self, evaluation_counter: UnigramCounter) -> float:\n",
    "        \"\"\"\n",
    "        Calculate the average log likelihood of the model on the evaluation text\n",
    "        :param evaluation_counter: unigram counter for the text on which the model is evaluated on\n",
    "        :return: average log likelihood that the unigram model assigns to the evaluation text\n",
    "        \"\"\"\n",
    "        test_log_likelihood = 0\n",
    "        test_counts = evaluation_counter.counts\n",
    "\n",
    "        for unigram, test_count in test_counts.items():\n",
    "            if unigram not in self.vocab:\n",
    "                unigram = '[UNK]'\n",
    "            train_prob = self.probs[unigram]\n",
    "            log_likelihood = test_count * log2(train_prob)\n",
    "            test_log_likelihood += log_likelihood\n",
    "\n",
    "        avg_test_log_likelihood = test_log_likelihood / evaluation_counter.token_count\n",
    "        return avg_test_log_likelihood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "para = \"Even if CB is not extended, like what you said I doubt things will go back to normal so quickly. If anything, I hope that the government implements some kind of stage by stage lifting of the CB measures. God knows what's going to happen when govt lifts CB in one go. Can you imagine? Hi, so as long as it is cohabitation for the full one month? I'm wondering if measures will be put into place to catch people not residing in their home address? This is -- perfect!\"\n",
    "\n",
    "text1 = \"Why is TCM included in the first wave of things to be allowed to open? Genuinely curious. See you guys after 29 days guys! Meanwhile, please stay safe, stay healthy &amp; STAY AT HOME! We shall see who gains the most weight(probably me heheh) after this circuit breaker thingy ü§™ü§™ #ayuatwork #mon√©quipe @‚Ä¶ https://t.co/9aJ9l45Rwn.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "['Even,if,CB,is,not,extended,like,what,you,said,I,doubt,things,will,go,back,to,normal,so,quickly,[END]', 'If,anything,I,hope,that,the,government,implements,some,kind,of,stage,by,stage,lifting,of,the,CB,measures,[END]', \"God,knows,what's,going,to,happen,when,govt,lifts,CB,in,one,go,[END]\", 'Can,you,imagine,[END]', 'Hi,so,as,long,as,it,is,cohabitation,for,the,full,one,month,[END]', \"I'm,wondering,if,measures,will,be,put,into,place,to,catch,people,not,residing,in,their,home,address,[END]\", 'This,is,perfect,[END]']\n"
     ]
    }
   ],
   "source": [
    "para_replaced = replace_characters(para)\n",
    "txt = []\n",
    "\n",
    "for tokenized_sentence in generate_tokenized_sentences(para_replaced):\n",
    "    sent = ','.join(tokenized_sentence)\n",
    "    txt.append(sent)\n",
    "print(txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_counter = UnigramCounter(txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "{'Even': 1, 'if': 2, 'CB': 3, 'is': 3, 'not': 2, 'extended': 1, 'like': 1, 'what': 1, 'you': 2, 'said': 1, 'I': 2, 'doubt': 1, 'things': 1, 'will': 2, 'go': 2, 'back': 1, 'to': 3, 'normal': 1, 'so': 2, 'quickly': 1, '[END]': 7, 'If': 1, 'anything': 1, 'hope': 1, 'that': 1, 'the': 3, 'government': 1, 'implements': 1, 'some': 1, 'kind': 1, 'of': 2, 'stage': 2, 'by': 1, 'lifting': 1, 'measures': 2, 'God': 1, 'knows': 1, \"what's\": 1, 'going': 1, 'happen': 1, 'when': 1, 'govt': 1, 'lifts': 1, 'in': 2, 'one': 2, 'Can': 1, 'imagine': 1, 'Hi': 1, 'as': 2, 'long': 1, 'it': 1, 'cohabitation': 1, 'for': 1, 'full': 1, 'month': 1, \"I'm\": 1, 'wondering': 1, 'be': 1, 'put': 1, 'into': 1, 'place': 1, 'catch': 1, 'people': 1, 'residing': 1, 'their': 1, 'home': 1, 'address': 1, 'This': 1, 'perfect': 1}\n"
     ]
    }
   ],
   "source": [
    "print(train_counter.counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "{'Even': 0.012048192771084338, 'if': 0.018072289156626505, 'CB': 0.024096385542168676, 'is': 0.024096385542168676, 'not': 0.018072289156626505, 'extended': 0.012048192771084338, 'like': 0.012048192771084338, 'what': 0.012048192771084338, 'you': 0.018072289156626505, 'said': 0.012048192771084338, 'I': 0.018072289156626505, 'doubt': 0.012048192771084338, 'things': 0.012048192771084338, 'will': 0.018072289156626505, 'go': 0.018072289156626505, 'back': 0.012048192771084338, 'to': 0.024096385542168676, 'normal': 0.012048192771084338, 'so': 0.018072289156626505, 'quickly': 0.012048192771084338, '[END]': 0.04819277108433735, 'If': 0.012048192771084338, 'anything': 0.012048192771084338, 'hope': 0.012048192771084338, 'that': 0.012048192771084338, 'the': 0.024096385542168676, 'government': 0.012048192771084338, 'implements': 0.012048192771084338, 'some': 0.012048192771084338, 'kind': 0.012048192771084338, 'of': 0.018072289156626505, 'stage': 0.018072289156626505, 'by': 0.012048192771084338, 'lifting': 0.012048192771084338, 'measures': 0.018072289156626505, 'God': 0.012048192771084338, 'knows': 0.012048192771084338, \"what's\": 0.012048192771084338, 'going': 0.012048192771084338, 'happen': 0.012048192771084338, 'when': 0.012048192771084338, 'govt': 0.012048192771084338, 'lifts': 0.012048192771084338, 'in': 0.018072289156626505, 'one': 0.018072289156626505, 'Can': 0.012048192771084338, 'imagine': 0.012048192771084338, 'Hi': 0.012048192771084338, 'as': 0.018072289156626505, 'long': 0.012048192771084338, 'it': 0.012048192771084338, 'cohabitation': 0.012048192771084338, 'for': 0.012048192771084338, 'full': 0.012048192771084338, 'month': 0.012048192771084338, \"I'm\": 0.012048192771084338, 'wondering': 0.012048192771084338, 'be': 0.012048192771084338, 'put': 0.012048192771084338, 'into': 0.012048192771084338, 'place': 0.012048192771084338, 'catch': 0.012048192771084338, 'people': 0.012048192771084338, 'residing': 0.012048192771084338, 'their': 0.012048192771084338, 'home': 0.012048192771084338, 'address': 0.012048192771084338, 'This': 0.012048192771084338, 'perfect': 0.012048192771084338, '[UNK]': 0.006024096385542169}\n"
     ]
    }
   ],
   "source": [
    "train_model = UnigramModel(train_counter)\n",
    "train_model.train(k=1)\n",
    "print(train_model.probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "text1_replaced = replace_characters(text1)\n",
    "txt1 = ''\n",
    "\n",
    "for tokenized_sentence in generate_tokenized_sentences(text1_replaced):\n",
    "    sent = ','.join(tokenized_sentence)\n",
    "    txt1 += sent\n",
    "\n",
    "test1_counter = UnigramCounter(txt1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "text1_avg_log_likelihood = train_model.evaluate(test1_counter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "-7.375039431346924"
      ]
     },
     "metadata": {},
     "execution_count": 39
    }
   ],
   "source": [
    "text1_avg_log_likelihood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "source": [
    "# News corpus unigram model"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"/Users/chenjianyu/Downloads/News Article/articles1.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "   Unnamed: 0     id                                              title  \\\n",
       "0           0  17283  House Republicans Fret About Winning Their Hea...   \n",
       "1           1  17284  Rift Between Officers and Residents as Killing...   \n",
       "2           2  17285  Tyrus Wong, ‚ÄòBambi‚Äô Artist Thwarted by Racial ...   \n",
       "3           3  17286  Among Deaths in 2016, a Heavy Toll in Pop Musi...   \n",
       "4           4  17287  Kim Jong-un Says North Korea Is Preparing to T...   \n",
       "\n",
       "      publication                         author        date    year  month  \\\n",
       "0  New York Times                     Carl Hulse  2016-12-31  2016.0   12.0   \n",
       "1  New York Times  Benjamin Mueller and Al Baker  2017-06-19  2017.0    6.0   \n",
       "2  New York Times                   Margalit Fox  2017-01-06  2017.0    1.0   \n",
       "3  New York Times               William McDonald  2017-04-10  2017.0    4.0   \n",
       "4  New York Times                  Choe Sang-Hun  2017-01-02  2017.0    1.0   \n",
       "\n",
       "   url                                            content  \n",
       "0  NaN  WASHINGTON  ‚Äî   Congressional Republicans have...  \n",
       "1  NaN  After the bullet shells get counted, the blood...  \n",
       "2  NaN  When Walt Disney‚Äôs ‚ÄúBambi‚Äù opened in 1942, cri...  \n",
       "3  NaN  Death may be the great equalizer, but it isn‚Äôt...  \n",
       "4  NaN  SEOUL, South Korea  ‚Äî   North Korea‚Äôs leader, ...  "
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Unnamed: 0</th>\n      <th>id</th>\n      <th>title</th>\n      <th>publication</th>\n      <th>author</th>\n      <th>date</th>\n      <th>year</th>\n      <th>month</th>\n      <th>url</th>\n      <th>content</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>17283</td>\n      <td>House Republicans Fret About Winning Their Hea...</td>\n      <td>New York Times</td>\n      <td>Carl Hulse</td>\n      <td>2016-12-31</td>\n      <td>2016.0</td>\n      <td>12.0</td>\n      <td>NaN</td>\n      <td>WASHINGTON  ‚Äî   Congressional Republicans have...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1</td>\n      <td>17284</td>\n      <td>Rift Between Officers and Residents as Killing...</td>\n      <td>New York Times</td>\n      <td>Benjamin Mueller and Al Baker</td>\n      <td>2017-06-19</td>\n      <td>2017.0</td>\n      <td>6.0</td>\n      <td>NaN</td>\n      <td>After the bullet shells get counted, the blood...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2</td>\n      <td>17285</td>\n      <td>Tyrus Wong, ‚ÄòBambi‚Äô Artist Thwarted by Racial ...</td>\n      <td>New York Times</td>\n      <td>Margalit Fox</td>\n      <td>2017-01-06</td>\n      <td>2017.0</td>\n      <td>1.0</td>\n      <td>NaN</td>\n      <td>When Walt Disney‚Äôs ‚ÄúBambi‚Äù opened in 1942, cri...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>3</td>\n      <td>17286</td>\n      <td>Among Deaths in 2016, a Heavy Toll in Pop Musi...</td>\n      <td>New York Times</td>\n      <td>William McDonald</td>\n      <td>2017-04-10</td>\n      <td>2017.0</td>\n      <td>4.0</td>\n      <td>NaN</td>\n      <td>Death may be the great equalizer, but it isn‚Äôt...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>4</td>\n      <td>17287</td>\n      <td>Kim Jong-un Says North Korea Is Preparing to T...</td>\n      <td>New York Times</td>\n      <td>Choe Sang-Hun</td>\n      <td>2017-01-02</td>\n      <td>2017.0</td>\n      <td>1.0</td>\n      <td>NaN</td>\n      <td>SEOUL, South Korea  ‚Äî   North Korea‚Äôs leader, ...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 48
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[['content','id', 'publication']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = []\n",
    "\n",
    "for i, row in df.iterrows():\n",
    "    article = row['content']\n",
    "\n",
    "    article_replaced = replace_characters(article)\n",
    "\n",
    "    for tokenized_sentence in generate_tokenized_sentences(article_replaced):\n",
    "        s = ','.join(tokenized_sentence)\n",
    "        corpus.append(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_counter = UnigramCounter(corpus)\n",
    "\n",
    "train_model = UnigramModel(train_counter)\n",
    "train_model.train(k=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "243313"
      ]
     },
     "metadata": {},
     "execution_count": 75
    }
   ],
   "source": [
    "train_model.vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from thoughtfulness import *\n",
    "\n",
    "news_unigram = news_articles_unigram(\"./Data/News Article/articles1.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "243313"
      ]
     },
     "metadata": {},
     "execution_count": 4
    }
   ],
   "source": [
    "news_unigram.vocab_size"
   ]
  },
  {
   "source": [
    "# Comment length"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "comments = pd.read_excel(\"./Data/Thoughtful Comments/Thoughtful Comments Labeled.xlsx\", engine='openpyxl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def remove_emoji(text):\n",
    "    emoji_pattern = re.compile(\"[\"\n",
    "    u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "    u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "    u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "    u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "    u\"\\U00002702-\\U000027B0\"\n",
    "    u\"\\U000024C2-\\U0001F251\"\n",
    "    \"]+\", flags=re.UNICODE)\n",
    "\n",
    "    return emoji_pattern.sub(r'', text)\n",
    "\n",
    "def remove_hashtag_mentions_urls(text):\n",
    "    return re.sub(r\"(?:\\@|\\#|https?\\://)\\S+\", \"\", text)\n",
    "\n",
    "\n",
    "# Thoughtful comment feature 1\n",
    "def get_sentence_length(text: str) -> int:\n",
    "    text = replace_characters(text)\n",
    "    text = remove_hashtag_mentions_urls(text)\n",
    "    text = remove_emoji(text)\n",
    "\n",
    "    word_tokenizer = RegexpTokenizer(r'[-\\'\\w]+')\n",
    "    tokenized_text = word_tokenizer.tokenize(text)\n",
    "\n",
    "    return len(tokenized_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "comments_len = []\n",
    "for i, row in comments.iterrows():\n",
    "    comment = row['Comment']\n",
    "\n",
    "    comment_len = get_sentence_length(comment)\n",
    "\n",
    "    comments_len.append(comment_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "comments['length'] = comments_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "comments.to_csv(\"./Data/Thoughtful Comments/Thoughtful Comments Features.csv\")"
   ]
  },
  {
   "source": [
    "# Comment loglikelihood"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "comments = pd.read_csv(\"./Data/Thoughtful Comments/Thoughtful Comments Features.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "comments.drop(comments[comments.length == 0].index, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "comments_likelihood = []\n",
    "for i, row in comments.iterrows():\n",
    "    comment = row['Comment']\n",
    "    \n",
    "    comment_avg_loglikelihood = comment_likelihood(comment, news_unigram)\n",
    "\n",
    "    comments_likelihood.append(comment_avg_loglikelihood)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "comments['Avg_loglikelihood'] = comments_likelihood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "   Unnamed: 0                                            Comment  Thoughtful?  \\\n",
       "0           0  Herd immunity kicks in at around 70% so we don...            1   \n",
       "1           1  straight up get struck by lightning better.\\n ...            0   \n",
       "2           2  The guide to getting the COVID-19 vaccine out ...            0   \n",
       "3           3           Other countries sud learn from Singapore            0   \n",
       "4           4  Catholic priest developing COVID-19 vaccine fo...            0   \n",
       "\n",
       "   length  Avg_loglikelihood  \n",
       "0      65         -18.838243  \n",
       "1       7         -20.329141  \n",
       "2      11         -17.852038  \n",
       "3       6         -18.359116  \n",
       "4       8         -18.318246  "
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Unnamed: 0</th>\n      <th>Comment</th>\n      <th>Thoughtful?</th>\n      <th>length</th>\n      <th>Avg_loglikelihood</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>Herd immunity kicks in at around 70% so we don...</td>\n      <td>1</td>\n      <td>65</td>\n      <td>-18.838243</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1</td>\n      <td>straight up get struck by lightning better.\\n ...</td>\n      <td>0</td>\n      <td>7</td>\n      <td>-20.329141</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2</td>\n      <td>The guide to getting the COVID-19 vaccine out ...</td>\n      <td>0</td>\n      <td>11</td>\n      <td>-17.852038</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>3</td>\n      <td>Other countries sud learn from Singapore</td>\n      <td>0</td>\n      <td>6</td>\n      <td>-18.359116</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>4</td>\n      <td>Catholic priest developing COVID-19 vaccine fo...</td>\n      <td>0</td>\n      <td>8</td>\n      <td>-18.318246</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 39
    }
   ],
   "source": [
    "comments.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "      Unnamed: 0                                            Comment  \\\n",
       "0              0  Herd immunity kicks in at around 70% so we don...   \n",
       "1              1  straight up get struck by lightning better.\\n ...   \n",
       "2              2  The guide to getting the COVID-19 vaccine out ...   \n",
       "3              3           Other countries sud learn from Singapore   \n",
       "4              4  Catholic priest developing COVID-19 vaccine fo...   \n",
       "...          ...                                                ...   \n",
       "1995        1995  Day 11 of circuit breaker period. #safedistanc...   \n",
       "1996        1996  Day 6 of circuit breaker: I have an air freshe...   \n",
       "1997        1997  FYI, Starbucks is opened, I guess CBTL is too....   \n",
       "1998        1998  I'm expecting to see dormitory cases skyrocket...   \n",
       "1999        1999  @VFSGlobal Yesterday singapore prime minister ...   \n",
       "\n",
       "      Thoughtful?  Length  Avg_loglikelihood  \n",
       "0               1      65         -18.838243  \n",
       "1               0       7         -20.329141  \n",
       "2               0      11         -17.852038  \n",
       "3               0       6         -18.359116  \n",
       "4               0       8         -18.318246  \n",
       "...           ...     ...                ...  \n",
       "1995            0      18         -18.061384  \n",
       "1996            0      24         -18.815028  \n",
       "1997            0      26         -18.604685  \n",
       "1998            0      62         -18.520687  \n",
       "1999            0      33         -18.859656  \n",
       "\n",
       "[1963 rows x 5 columns]"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Unnamed: 0</th>\n      <th>Comment</th>\n      <th>Thoughtful?</th>\n      <th>Length</th>\n      <th>Avg_loglikelihood</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>Herd immunity kicks in at around 70% so we don...</td>\n      <td>1</td>\n      <td>65</td>\n      <td>-18.838243</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1</td>\n      <td>straight up get struck by lightning better.\\n ...</td>\n      <td>0</td>\n      <td>7</td>\n      <td>-20.329141</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2</td>\n      <td>The guide to getting the COVID-19 vaccine out ...</td>\n      <td>0</td>\n      <td>11</td>\n      <td>-17.852038</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>3</td>\n      <td>Other countries sud learn from Singapore</td>\n      <td>0</td>\n      <td>6</td>\n      <td>-18.359116</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>4</td>\n      <td>Catholic priest developing COVID-19 vaccine fo...</td>\n      <td>0</td>\n      <td>8</td>\n      <td>-18.318246</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>1995</th>\n      <td>1995</td>\n      <td>Day 11 of circuit breaker period. #safedistanc...</td>\n      <td>0</td>\n      <td>18</td>\n      <td>-18.061384</td>\n    </tr>\n    <tr>\n      <th>1996</th>\n      <td>1996</td>\n      <td>Day 6 of circuit breaker: I have an air freshe...</td>\n      <td>0</td>\n      <td>24</td>\n      <td>-18.815028</td>\n    </tr>\n    <tr>\n      <th>1997</th>\n      <td>1997</td>\n      <td>FYI, Starbucks is opened, I guess CBTL is too....</td>\n      <td>0</td>\n      <td>26</td>\n      <td>-18.604685</td>\n    </tr>\n    <tr>\n      <th>1998</th>\n      <td>1998</td>\n      <td>I'm expecting to see dormitory cases skyrocket...</td>\n      <td>0</td>\n      <td>62</td>\n      <td>-18.520687</td>\n    </tr>\n    <tr>\n      <th>1999</th>\n      <td>1999</td>\n      <td>@VFSGlobal Yesterday singapore prime minister ...</td>\n      <td>0</td>\n      <td>33</td>\n      <td>-18.859656</td>\n    </tr>\n  </tbody>\n</table>\n<p>1963 rows √ó 5 columns</p>\n</div>"
     },
     "metadata": {},
     "execution_count": 40
    }
   ],
   "source": [
    "comments.rename(columns={'length':'Length'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "comments.to_csv(\"./Data/Thoughtful Comments/Thoughtful Comments Features.csv\")"
   ]
  },
  {
   "source": [
    "# Number of verbs"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "comments = pd.read_csv(\"./Data/Thoughtful Comments/Thoughtful Comments Features.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "verb_tags = ['VB', 'VBD', 'VBG', 'VBN', 'VBP', 'VBZ']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_verbs = []\n",
    "for i, row in comments.iterrows():\n",
    "    text = row['Comment']\n",
    "\n",
    "    text = replace_characters(text)\n",
    "    text = remove_hashtag_mentions_urls(text)\n",
    "    text = remove_emoji(text)\n",
    "\n",
    "    word_tokenizer = RegexpTokenizer(r'[-\\'\\w]+')\n",
    "    tokenized_text = word_tokenizer.tokenize(text)\n",
    "\n",
    "    comment_tags = nltk.pos_tag(tokenized_text)\n",
    "\n",
    "    count = 0\n",
    "    for tag in comment_tags:\n",
    "        if tag[1] in verb_tags:\n",
    "            count += 1\n",
    "    \n",
    "    num_verbs.append(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "comments['Num verbs'] = num_verbs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "comments.drop(columns=['Unnamed: 0', 'Unnamed: 0.1'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "comments.to_csv(\"./Data/Thoughtful Comments/Thoughtful Comments Features.csv\")"
   ]
  },
  {
   "source": [
    "# Number of discourse relations"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = 'although, as though, but, by comparison, even if, even though, however, nevertheless, on the other hand, still, then, though, while, yet, and, meanwhile, in turn, next, ultimately, meantime, also, as if, even as, even still, even then, regardless, when, by contrast, conversely, if, in contrast, instead, nor, or, rather, whereas, while, yet, even after, by contrast, nevertheless, besides, much as, as much as, whereas, neither, nonetheless, even when, on the one hand indeed, finally, in fact, separately, in the end, on the contrary, while, accordingly, additionally, after, also although, and, as, as it, as if besides, but, by comparison, finally, first, for example, for one thing, however, in addition, in fact, in other words, in particular, in response, in sum, in the end, in turn, incidentally, indeed, instead, likewise, meanwhile, nevertheless, on the one hand, on the whole, overall, plus, separately, much as, whereas, ultimately, as though, rather, at the same time, or, then, if, in turn, furthermore, in short, turns out, while, yet, that is, so, what‚Äôs more as a matter of fact, further, in return, moreover, similarly, specifically, and, when, typically, as long as, especially if, even if, even when, if, so, when if only, lest,once, only if, only when, particularly if, at least partly because, especially as, especially because, especially since, in large part because, just because, largely because, merely because, not because, not only because, particularly as, particularly because, particularly since, partly because, because, simply because, since, then, after, one day after, reportedly after, consequently, mainly because, for, thus, apparently, in the end, in turn, primarily because, largely as a result, as, because, therefore, only because, particularly, when, so that, thereby, presumably, hence, as a result, if and when, unless, until, in part because, now that, perhaps because, only after, accordingly'\n",
    "\n",
    "x = x.split(', ')\n",
    "x = list(set(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, ele in enumerate(x):\n",
    "    x[i] = ' ' + ele + ' '"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[' what‚Äôs more as a matter of fact ',\n",
       " ' at least partly because ',\n",
       " ' on the one hand indeed ',\n",
       " ' in large part because ',\n",
       " ' particularly because ',\n",
       " ' largely as a result ',\n",
       " ' particularly since ',\n",
       " ' especially because ',\n",
       " ' on the other hand ',\n",
       " ' primarily because ',\n",
       " ' at the same time ',\n",
       " ' especially since ',\n",
       " ' not only because ',\n",
       " ' reportedly after ',\n",
       " ' on the contrary ',\n",
       " ' in part because ',\n",
       " ' largely because ',\n",
       " ' particularly as ',\n",
       " ' perhaps because ',\n",
       " ' particularly if ',\n",
       " ' on the one hand ',\n",
       " ' simply because ',\n",
       " ' merely because ',\n",
       " ' in other words ',\n",
       " ' partly because ',\n",
       " ' mainly because ',\n",
       " ' in particular ',\n",
       " ' for one thing ',\n",
       " ' as if besides ',\n",
       " ' one day after ',\n",
       " ' especially as ',\n",
       " ' by comparison ',\n",
       " ' especially if ',\n",
       " ' also although ',\n",
       " ' consequently ',\n",
       " ' on the whole ',\n",
       " ' when if only ',\n",
       " ' particularly ',\n",
       " ' incidentally ',\n",
       " ' just because ',\n",
       " ' only because ',\n",
       " ' nevertheless ',\n",
       " ' specifically ',\n",
       " ' additionally ',\n",
       " ' not because ',\n",
       " ' for example ',\n",
       " ' in addition ',\n",
       " ' accordingly ',\n",
       " ' furthermore ',\n",
       " ' in response ',\n",
       " ' by contrast ',\n",
       " ' if and when ',\n",
       " ' in contrast ',\n",
       " ' nonetheless ',\n",
       " ' even though ',\n",
       " ' as a result ',\n",
       " ' separately ',\n",
       " ' in the end ',\n",
       " ' conversely ',\n",
       " ' as long as ',\n",
       " ' regardless ',\n",
       " ' ultimately ',\n",
       " ' even after ',\n",
       " ' as much as ',\n",
       " ' apparently ',\n",
       " ' presumably ',\n",
       " ' only after ',\n",
       " ' even still ',\n",
       " ' in return ',\n",
       " ' only when ',\n",
       " ' even then ',\n",
       " ' lest,once ',\n",
       " ' even when ',\n",
       " ' turns out ',\n",
       " ' as though ',\n",
       " ' similarly ',\n",
       " ' therefore ',\n",
       " ' typically ',\n",
       " ' meanwhile ',\n",
       " ' likewise ',\n",
       " ' moreover ',\n",
       " ' in short ',\n",
       " ' now that ',\n",
       " ' meantime ',\n",
       " ' although ',\n",
       " ' instead ',\n",
       " ' that is ',\n",
       " ' much as ',\n",
       " ' only if ',\n",
       " ' in fact ',\n",
       " ' even as ',\n",
       " ' neither ',\n",
       " ' further ',\n",
       " ' overall ',\n",
       " ' even if ',\n",
       " ' besides ',\n",
       " ' so that ',\n",
       " ' because ',\n",
       " ' in turn ',\n",
       " ' finally ',\n",
       " ' whereas ',\n",
       " ' thereby ',\n",
       " ' however ',\n",
       " ' indeed ',\n",
       " ' though ',\n",
       " ' unless ',\n",
       " ' rather ',\n",
       " ' in sum ',\n",
       " ' after ',\n",
       " ' until ',\n",
       " ' still ',\n",
       " ' while ',\n",
       " ' hence ',\n",
       " ' since ',\n",
       " ' first ',\n",
       " ' as if ',\n",
       " ' as it ',\n",
       " ' also ',\n",
       " ' when ',\n",
       " ' next ',\n",
       " ' thus ',\n",
       " ' plus ',\n",
       " ' then ',\n",
       " ' but ',\n",
       " ' yet ',\n",
       " ' nor ',\n",
       " ' for ',\n",
       " ' and ',\n",
       " ' if ',\n",
       " ' or ',\n",
       " ' as ',\n",
       " ' so ']"
      ]
     },
     "metadata": {},
     "execution_count": 98
    }
   ],
   "source": [
    "x.sort(key = lambda x: len(x), reverse=True)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "comments = pd.read_csv(\"./Data/Thoughtful Comments/Thoughtful Comments Features.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "num_discourse = []\n",
    "for i, row in comments.iterrows():\n",
    "    text = row['Comment']\n",
    "\n",
    "    text = remove_hashtag_mentions_urls(text)\n",
    "    text = remove_emoji(text)\n",
    "    text = replace_characters(text)\n",
    "\n",
    "    count = 0\n",
    "    for sentence in sent_tokenize(text):\n",
    "        word_tokenizer = RegexpTokenizer(r'[-\\'\\w]+')\n",
    "        tokenized_text = word_tokenizer.tokenize(sentence)\n",
    "        tokenized_text = [w.lower() for w in tokenized_text]\n",
    "\n",
    "        text_final = \" \".join(tokenized_text)\n",
    "\n",
    "        for ele in x:\n",
    "            if ele in text_final:\n",
    "                count += 1\n",
    "        \n",
    "    num_discourse.append(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "comments['Nume Discourse Relations'] = num_discourse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "comments.to_csv(\"./Data/Thoughtful Comments/Thoughtful Comments Features.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}